{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "import random\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical #short-cut to one hot encode the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 13s 1us/step\n"
     ]
    }
   ],
   "source": [
    "(train_images,train_labels),(test_images,test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (10000, 28, 28))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape,test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBxmeJtv9WSKzAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11f228908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Show the second image of the training dataset\n",
    "plt.imshow(train_images[1],cmap='Greys')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self,sizes):\n",
    "        \"\"\"\n",
    "        This is the init method for the neural network which will be evaluating mnist using adaptive learning rates \n",
    "        and clr.\n",
    "        The size of the neural network will be the length of the list sizes.\n",
    "        for example if sizes is [2,3,1] it will be a 3 layered neural network with the first layer being the input layer\n",
    "        and containing 2 neurons with a hidden layer of 3 neurons and an output layer of 1 neuron\n",
    "        The bias and weights of the network will be initialized with a gaussian distribution with mean of zero and\n",
    "        variance 1 basically np.random.rand :P\n",
    "        \n",
    "        \"\"\"\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "    def sigmoid(z):\n",
    "        \"\"\"\n",
    "        The sigmoid activation function\n",
    "        \"\"\"\n",
    "        return 1.0/(1.0+np.exp(-z))\n",
    "    def sigmoid_prime(z):\n",
    "        \"\"\"\n",
    "        The derivative of sigmoid used in backpropagation\n",
    "        \"\"\"\n",
    "        return sigmoid(z)*(1-sigmoid(z))\n",
    "    def feedforward_nn(self,X):\n",
    "        \"\"\"\n",
    "        This is the forward propagation function of the neural network.\n",
    "        The equation is Input times weight, add a bias, activate.\n",
    "        \"\"\"\n",
    "        for W,b in zip(self.weights,self.biases):\n",
    "            \n",
    "            Z = sigmoid(np.dot(W,x) + b)\n",
    "        print(Z.shape) #verify the shape of Z\n",
    "        return Z\n",
    "    \n",
    "    def get_triangularLR(iteration,stepSize,base_lr,max_lr):\n",
    "        \"\"\"\n",
    "        Define the learning rate according to the CLR technique. Define the total no of iterations\n",
    "        and vary the learning rate in between the iterations in 1 epoch\n",
    "        \"\"\"\n",
    "        cycle = np.floor(1+iteration/2*stepSize)\n",
    "        x = np.abs(iteration/stepsize - 2*cycle + 1)\n",
    "        lr = base_lr + (max_lr - base_lr) * np.maximum(0, (1-x))\n",
    "        return lr\n",
    "    \n",
    "    def SGD(self,training_data,epochs,mini_batch_size):\n",
    "        \"\"\"\n",
    "        Train the neural network using stochastic gradient descent.\n",
    "        Divide the training_data passed into mini batches.\n",
    "        For each mini_batch update the gradient and need to update the weights somewhere\n",
    "        \"\"\"\n",
    "        n = len(training_data)\n",
    "        stepsize = np.ceil(n/mini_batch_size)\n",
    "        base_lr = 0.01\n",
    "        max_lr = 0.08\n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in range(0, n, mini_batch_size)]\n",
    "            \n",
    "            #get_triangularCR\n",
    "            for index,mini_batch in enumerate(mini_batches):\n",
    "                iteration = index + (j-1)*stepsize\n",
    "                lr = get_triangularLR(iteration,stepsize,base_lr,max_lr)\n",
    "                self.update_mini_batch(mini_batch,lr)\n",
    "            \n",
    "\n",
    "    def backprop(self):\n",
    "        \"\"\"\n",
    "        Backpropagate to update weights\n",
    "        \"\"\"\n",
    "        pass\n",
    "    def update_mini_batch(self,mini_batch,eta):\n",
    "        \"\"\"Update the network's weights and biases by applying\n",
    "        gradient descent using backpropagation to a single mini batch.\n",
    "        The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta``\n",
    "        is the learning rate.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x,y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [w-(eta/len(mini_batch))*nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb\n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_triangularLR(iteration,stepsize,base_lr,max_lr):\n",
    "    cycle = np.floor(1+iteration/2*stepSize)\n",
    "    print(\"cycle {}\".format(cycle))\n",
    "    x = np.abs(iteration/stepsize - 2*cycle + 1)\n",
    "    print(\"x {}\".format(x))\n",
    "    #lr = base_lr + (max_lr - base_lr) * np.maximum(0, (1-x))\n",
    "    lr = base_lr + (max_lr-base_lr)*np.maximum(0, (1-x))\n",
    "    print(np.maximum(0,(1-x)))\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "cycle 1.0\n",
      "x 1.0\n",
      "0.0\n",
      "1\n",
      "cycle 1.0\n",
      "x 0.0\n",
      "1.0\n",
      "2\n",
      "cycle 2.0\n",
      "x 1.0\n",
      "0.0\n",
      "3\n",
      "cycle 2.0\n",
      "x 0.0\n",
      "1.0\n",
      "4\n",
      "cycle 3.0\n",
      "x 1.0\n",
      "0.0\n",
      "5\n",
      "cycle 3.0\n",
      "x 0.0\n",
      "1.0\n",
      "6\n",
      "cycle 4.0\n",
      "x 1.0\n",
      "0.0\n",
      "7\n",
      "cycle 4.0\n",
      "x 0.0\n",
      "1.0\n",
      "8\n",
      "cycle 5.0\n",
      "x 1.0\n",
      "0.0\n",
      "9\n",
      "cycle 5.0\n",
      "x 0.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "num_iterations = 10\n",
    "stepSize = 1\n",
    "base_lr = 0.001\n",
    "max_lr = 0.01\n",
    "lr_trend = list()\n",
    "for iteration in range(num_iterations):\n",
    "    print(iteration)\n",
    "    lr = get_triangularLR(iteration, stepSize, base_lr, max_lr)\n",
    "    \n",
    "    # Update your optimizer to use this learning rate in this iteration\n",
    "    lr_trend.append(lr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'! nuf si sihT'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = \"This is fun !\"\n",
    "string[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.001,\n",
       " 0.010000000000000002,\n",
       " 0.001,\n",
       " 0.010000000000000002,\n",
       " 0.001,\n",
       " 0.010000000000000002,\n",
       " 0.001,\n",
       " 0.010000000000000002,\n",
       " 0.001,\n",
       " 0.010000000000000002]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sizes = [2,3,1]\n",
    "weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bias = [np.random.randn(y, 1) for y in sizes[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.09863083],\n",
       "       [-0.1345898 ],\n",
       "       [ 0.96140015]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randn(3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.1600541 ],\n",
       "        [-0.73433089],\n",
       "        [-0.14546473]]), array([[-0.53342218]])]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input = np.random.randn(2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.20653489],\n",
       "       [-1.47811513]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Z = weights[0].dot(input) + bias[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (1,3) and (2,1) not aligned: 3 (dim 1) != 2 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-d2c0184c555e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mZ1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mZ1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: shapes (1,3) and (2,1) not aligned: 3 (dim 1) != 2 (dim 0)"
     ]
    }
   ],
   "source": [
    "Z1 = []\n",
    "for W,b in zip(weights,bias):\n",
    "    Z1 = np.dot(W,input)+b\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = len(train_images)\n",
    "training_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mini_batches = [training_data[k:k+64] for k in range(0,n,64)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "938"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mini_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "for index,mini_batch in enumerate(mini_batches):\n",
    "    print(mini_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "938.0"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ceil(60000/64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda]",
   "language": "python",
   "name": "conda-env-anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
